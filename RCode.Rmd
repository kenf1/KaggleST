---
title: "Title"
author: "KF"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
    code_folding: show
editor_options: 
  chunk_output_type: console
---

```{r setup,include=FALSE}
knitr::opts_chunk$set(
	echo=TRUE,
	message=FALSE,
	warning=FALSE,
	eval=FALSE
)
```

## Import

### Packages + Functions

```{r}
#Core packages/functions
gc()
library(tidyverse)

#Additional packages
library(naniar)
library(mice)
library(glmnet)
library(caret)
```

### Data

Load in train.csv file

```{r}
#located inside proj folder
df_train <- read_csv("~/Desktop/KaggleData/SpaceTitanic/train.csv")
```

## EDA

### Missingness

visualize it

```{r}
#vis_miss(df_train)
```

see not too much missingness (~3%)

now I want to fill in the missingness using imputation

## Feature Engineering

before I impute to fill in the NA cells, I want to perform a few feature engineering changes to improve the data imputation algorithm gets to work w/

### Cabin

currently in format of deck/num/side, plan to split into 3 separate cols

```{r}
#split into 3 separate cols
df_train <- df_train %>% separate(Cabin,c("CabinDeck","CabinNum","CabinSide"))
```

str_split on "/" also works

### PassengerID

see if in same group
  - take 1st 4 digits of passenger ID

```{r}
#extract 1st 4 digits
df_train$groupID <- str_extract_all(df_train$PassengerId,"\\d{4}")

#T/F for duplicates
df_train$sameGroup <- duplicated(df_train$groupID)

#replace 1st entry of dupes w/ TRUE

#extract groupID of dupes
df_train_dupeID <- df_train[,c("groupID","sameGroup")] %>% filter(sameGroup==TRUE)
df_train_dupeID <- unique(unlist(df_train_dupeID$groupID))

#fill in 1st entry of dupes
df_train$sameGroup <- ifelse(df_train$groupID %in% df_train_dupeID,
                              TRUE,df_train$sameGroup)
```

### Name

don't think much can be done for the names except:

1. guess the gender of the person
  -  match to common names
    - what to do for non-common names?
    - most of work has to be done manually? (worth trade-off?)

1. relatives/not
  - take surname & see if there are matches
  - binary: relative present, no relative present

```{r}
#doing same thing as for Cabin (will turn into function later on)
df_train <- df_train %>% separate(Name,c("FirstName","Surname"))

#T/F for duplicates
df_train$Relatives <- duplicated(df_train$Surname)

#replace 1st entry of dupes w/ TRUE

#extract groupID of dupes
df_train_relatives <- df_train[,c("Surname","Relatives")] %>% filter(Relatives==TRUE)
df_train_relatives <- unique(unlist(df_train_relatives$Surname))

#fill in 1st entry of dupes
df_train$Relatives <- ifelse(df_train$Surname %in% df_train_relatives,
                              TRUE,df_train$Relatives)

rm(df_train_dupeID,df_train_relatives)
```

***Note***: some names are NA, considered non-duplicate

### Remove Unnec Cols

```{r}
unnec_cols <- c("PassengerId","groupID","FirstName","Surname")

df_train <- df_train[,!names(df_train) %in% unnec_cols]

rm(unnec_cols)
```

## Imputation

to prepare for imputation, need to turn specific cols into factors

```{r}
#majority of char to factor
df_train[,c("HomePlanet","CabinDeck","CabinSide","Destination","VIP")] <- lapply(df_train[,c("HomePlanet","CabinDeck","CabinSide","Destination","VIP")],as.factor)

#cabin num to numeric
df_train$CabinNum <- as.numeric(df_train$CabinNum)
```

actual imputing step (takes ~ 1 minute)

```{r}
#impute only once (will try )
df_train_imp <- mice(df_train,m=1,maxit=1,seed=69420)
```

### Get Working Ex

```{r}
#obtain 1st imputed dataset
df_imp <- complete(df_train_imp,1)
```

## Model Fitting

### LASSO

```{r}
#rearrange col order
df_imp <- df_imp %>% relocate(Transported,.before=HomePlanet)

#create dataset
y <- df_imp$Transported
x <- data.matrix(df_imp[,-1])
```

#### Train model

***Warning***: takes ~15 min to train the initial model

```{r}
#model
cv_model <- cv.glmnet(x,y,family="binomial",alpha=1,type.measure="class",nfolds=nrow(df_imp))
```

##### Fine-tuning model

```{r}
#best lambda
best_lambda <- cv_model$lambda.1se

#model using best lambda
best_model <- glmnet(x,y,family="binomial",alpha=1,lambda=best_lambda)

#coefficients of best model
coef(best_model)
```

#### Test Model (on training dataset)

```{r}
#create predictions using model
predicted <- predict(best_model,as.matrix(x),type="response")

#convert predicted to y/n
predicted <- ifelse(predicted>0.5,"TRUE","FALSE")

#factorize
predicted <- factor(predicted)
df_imp$Transported <- factor(df_imp$Transported)

#confusion matrix
cm <- confusionMatrix(df_imp$Transported,predicted)
(cm)
```

balanced accuracy = `{r} cm$overall['Accuracy']`

## Test on Actual Test Dataset

```{r}
df_test <- read_csv("~/Desktop/KaggleData/SpaceTitanic/test.csv")
```

```{r}
vis_miss(df_test)
```

taking a look at the test dataset, it seems like we will need to 

## Clean global environ

```{r}
#rm
```

## Notes/Next Steps

1. Even with imputation set to 1 with 1 iteration, the LASSO model fitting step is still the most time-consuming.

1. Plan to try Decision Trees (with/without boosting)

1. Use 80/20 train/test split on `df_train`. Currently, I am using 100% of the dataset to train/test my model. This introduces bias and runs the risk of over-fitting.
